{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "n1ckfg-ExperimentalWriting",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi && free -h && python --version"
      ],
      "metadata": {
        "id": "teoMB63OrqPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GDRIVE\n",
        "!ls \"/content/drive/My Drive/COLAB/\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "rXQiCQ8yrrqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_"
      },
      "source": [
        "# DEPENDENCIES\n",
        "!pip install -q aitextgen\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "    display(HTML('''<style>pre { white-space: pre-wrap; }</style>'''))\n",
        "    \n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRhgNvsH4Wn"
      },
      "source": [
        "## Loading the GPT model\n",
        "\n",
        "Now we have to download the language model. This can take a while."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "457Oy52I9nwU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flqSlHjMIeIw"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "%cd \"/content\"\n",
        "!rm -rf aitextgen\n",
        "\n",
        "#model_selection=\"gpt-neo-125M\" # GPT-Neo Pytorch, requires 12GB VRAM\n",
        "model_selection=\"355M\" # Stock GPT-2 TF, requires 16GB VRAM\n",
        "#model_selection=\"124M\" # Stock GPT-2 TF, requires 8GB VRAM\n",
        "\n",
        "# 1. Do this first only if you have a local base model...\n",
        "!ln -s \"/content/drive/MyDrive/COLAB/ExperimentalWriting/base_models/$model_selection/aitextgen\"\n",
        "\n",
        "# 2. Do this next (for local or remote base models)...\n",
        "# 2.1. GPT-Neo Pytorch...\n",
        "#ai = aitextgen(model=\"EleutherAI/\" + model_selection, to_gpu=True) # gpt-neo-125M, gpt-neo-1.3B, gpt-neo-2.7B, gpt-neo-6B\n",
        "\n",
        "# 2.2. Stock GPT-2 TF...\n",
        "ai = aitextgen(tf_gpt2=model_selection, to_gpu=True) # 124M, 355M, 774M, 1558M\n",
        "\n",
        "# 2.3. Other custom Pytorch...\n",
        "#ai = aitextgen(model=model_selection, to_gpu=True) # gpt2-medium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "* **Upload** any text file, for example, a .txt file of your own writing, or a [pre-compiled corpus](https://discord.com/channels/928996422509027408/928996422509027414/931150499430924288). The file must be in UTF-8 format.\n",
        "* **Update** the file name in the cell below to *exactly* match the title of the .txt file as it exists in the sidebar folder, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "%cd \"/content\"\n",
        "!rm \"input.txt\"\n",
        "!ln -s \"/content/drive/MyDrive/COLAB/ExperimentalWriting/corpora/nick_corpus_combo.txt\" input.txt\n",
        "file_name = \"input.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetuning the language model\n",
        "The next cell will start the actual finetuning of the language model. It trains the model to the **voice and concepts** of the supplied text. \n",
        "\n",
        "Tuning runs for `num_steps` steps - **500** is a good number for experimenting. Longer training means that the model will be more sophisticated in how it combines your words with the words it already knows.\n",
        "\n",
        "A progress bar will appear to show training progress.\n",
        "\n",
        "***To begin with, just leave the settings as they are! As you learn and grow, you can mess around!***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- The model will be saved every `save_every` steps in `trained_model` by default, and when training completes.\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "- **`learning_rate`**: Learning rate of the model training. Only change this if you know what you're doing.\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. (if using `fp16`, you can increase the batch size more safely)\n",
        "\n",
        "Afterward, we reload the model so things are properly set up. \n",
        "\n",
        "Running `generate()` without any parameters generates a single text from the loaded model, to test that everything is in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf"
      },
      "source": [
        "ai.train(file_name,\n",
        "         line_by_line=False,\n",
        "         from_cache=False,\n",
        "         num_steps=500,\n",
        "         generate_every=100,\n",
        "         save_every=500,\n",
        "         save_gdrive=False,\n",
        "         learning_rate=1e-3,\n",
        "         fp16=False,\n",
        "         batch_size=1, \n",
        "         )\n",
        "\n",
        "ai = aitextgen(model_folder=\"trained_model\", to_gpu=True)\n",
        "\n",
        "ai.generate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "Now it gets interesting! You can pass a `prompt` to the generate function to force the text to start with a given phrase.\n",
        "\n",
        "**Be sure to never end the prompt with a space!**\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `n`. You can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, a `batch_size` of 50 will make you run out of memory).\n",
        "\n",
        "The following parameters define what you get back from the model:\n",
        "\n",
        "*  **`min_length`**: The minimum length of the generated text.\n",
        "*  **`max_length`**: Number of tokens to generate. Default is 256. You can generate up to 2048.\n",
        "* **`temperature`**: The higher the temperature, the crazier the text. Default is 0.7, recommended to keep between 0.7 and 1.0.\n",
        "* **`top_k`**: Filters the generated guesses to the top *k* guesses. Default is 0, which disables the behavior. If the generated output has semantic or syntactic issues, you may want to set `top_k=40` or use `top_p` sampling (see below).\n",
        "* **`top_p`**: Another way of filtering bad results calld _nucleus sampling_. Limits the generated guesses to a cumulative probability. Gets good results on a dataset with `top_p=0.9`. If `top_p` as well as `top_k` are specified only top_p is used.\n",
        "* **`repetition_penalty`**: Set this to more than 1 to avoid repeating words in the text.\n",
        "\n",
        "## Suggestions\n",
        "* gpt-neo-125M: \n",
        "temperature 1.35, top_p 0.97, repetition_penalty 1.1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "#@title Text Generation Parameters\n",
        "prompt = \"MARY: Get to battle stations!\" #@param {type:\"string\"}\n",
        "temperature = 1.35 #@param {type:\"slider\", min:0, max:1.5, step:0.05}\n",
        "top_p = 0.97 #@param {type:\"slider\", min:0.8, max:0.99, step:0.01}\n",
        "repetition_penalty = 1.1 #@param {type:\"slider\", min:1, max:1.5, step:0.1}\n",
        "batch_size = 3 #@param {type:\"slider\", min:1, max:3, step:1}\n",
        "max_length = 256 #@param {type:\"slider\", min:256, max:2048, step:256}\n",
        "\n",
        "ai.generate(n=batch_size,\n",
        "            batch_size=batch_size,\n",
        "            prompt=prompt,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature, \n",
        "            top_p=top_p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OUTPUT\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "# Saves the cached base model\n",
        "#!cp -r aitextgen /content/drive/MyDrive/COLAB/ExperimentalWriting/\n",
        "\n",
        "# Saves the training results\n",
        "!cp -r trained_model /content/drive/MyDrive/COLAB/ExperimentalWriting/"
      ],
      "metadata": {
        "id": "SDHSJg2U5Gy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Original work copyright (c) 2020-2021 Max Woolf\n",
        "\n",
        "Modified work copyright (c) 2022 Martin Pichlmair\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}